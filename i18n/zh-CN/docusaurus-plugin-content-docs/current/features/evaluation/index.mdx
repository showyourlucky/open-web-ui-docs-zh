---
sidebar_position: 6
title: "📝 评估"
---


## 我为什么要评估模型？

让我们认识一下**Alex**，他是一家中型公司的机器学习工程师。Alex知道市面上有很多AI模型——GPT、LLaMA等等——但哪一个最适合手头的任务呢？虽然这些模型在纸面上听起来都很厉害，但Alex不能仅仅依赖公开的排行榜。这些模型在不同的上下文中表现各异，有些模型可能在评估数据集上进行了训练（有点狡猾！）。此外，这些模型生成的文字有时感觉……不太对劲。

这时，Open WebUI就派上了用场。它为Alex和他的团队提供了一种简便的方式来根据实际需求评估模型。不需要复杂的数学计算，也不需要繁重的工作，只需在与模型互动时给出点赞或点踩即可。

### TL;DR

- **为什么评估很重要**：模型太多，但并非所有都适合你的具体需求。通用的公开排行榜不一定总是可信的。
- **如何解决**：Open WebUI提供了一个内置的评估系统。使用点赞或点踩来评价模型的回答。
- **幕后发生了什么**：评分会调整你个人的排行榜，而带有评分的聊天快照将用于未来的模型微调！
- **评估选项**：
  - **竞技场模式**：随机选择模型供你比较。
  - **常规互动**：像平时一样聊天并评价回答。

---

### 为什么公共评估不够？

- 公开排行榜没有针对**你的**具体用例进行定制。
- 一些模型是在评估数据集上训练的，这会影响结果的公平性。
- 某个模型整体表现很好，但其沟通风格或回答可能不符合你想要的“氛围”。

### 解决方案：使用Open WebUI进行个性化评估

Open WebUI有一个内置的评估功能，让你和你的团队能够通过与模型互动，发现最适合你们特定需求的模型。

它是如何工作的？很简单！

- **在聊天过程中**，如果你喜欢某个回答，可以点赞；如果不满意，则点踩。如果消息有**兄弟消息**（比如重新生成的回答或并排比较的不同模型的回答），你就是在为你的**个人排行榜**做贡献。
- **排行榜**可以在管理员部分轻松访问，帮助你跟踪哪些模型根据团队的表现最佳。

一个很酷的功能是，每当你评价一个回答时，系统会捕获该对话的**快照**，之后将用于改进模型甚至为未来模型的训练提供支持。（请注意，这个功能还在开发中！）

---

### 两种评估AI模型的方法

Open WebUI提供了两种简单直接的评估AI模型的方法。

### **1. 竞技场模式**

**竞技场模式**从可用模型池中随机选择模型，确保评估公平且无偏见。这有助于消除手动比较中的潜在缺陷：**生态有效性**——确保你不会有意或无意地偏向某个模型。

如何使用：
- 从竞技场模式选择器中选择一个模型。
- 像平时一样使用它，但现在你处于“竞技场模式”。

为了让你的反馈影响排行榜，你需要所谓的**兄弟消息**。什么是兄弟消息？兄弟消息就是由同一查询生成的替代回答（比如消息再生或多个模型并排生成的回答）。这样，你就可以进行**一对一的比较**。

- **评分提示**：当你给一个回答点赞时，另一个回答会自动得到点踩。所以，请谨慎选择，并只给真正最好的消息点赞！
- 一旦你评价了回答，你可以查看排行榜，看看各个模型的表现如何。

以下是竞技场模式界面的工作示例：

![竞技场模式示例](/img/evaluation/arena.png)

需要更多深度？你甚至可以复制一个[**Chatbot Arena**](https://lmarena.ai/)风格的设置！

![Chatbot Arena 示例](/img/evaluation/arena-many.png)

### **2. 常规互动**

如果你不想切换到“竞技场模式”，也可以正常使用Open WebUI并像平时操作一样评价AI模型的回答。只需点赞或点踩模型的回答，随时都可以。但是，**如果你想让反馈用于排行榜排名**，你需要**更换模型并与另一个模型互动**。这确保了有**兄弟回答**可供比较——只有两个不同模型之间的比较才会对排名产生影响。

例如，这是你在常规互动中如何进行评价的：

![常规模型评分界面](/img/evaluation/normal.png)

以及如何设置多模型比较，类似于竞技场：

![多模型比较](/img/evaluation/normal-many.png)

---

## 排行榜

在评分后，可以查看管理员面板中的**排行榜**。在这里，你会直观地看到模型的表现情况，使用的是**Elo评分系统**（类似国际象棋排名！）你会清楚了解哪些模型在评估中真正脱颖而出。

这是一个示例排行榜布局：

![排行榜示例](/img/evaluation/leaderboard.png)

### 按主题重新排名

当你评价聊天时，可以根据主题**标记它们**以获得更细致的见解。这对于在不同领域工作的人特别有用，比如**客户服务、创意写作、技术支持**等。

#### 自动标记
Open WebUI尝试根据对话主题**自动标记聊天**。然而，根据你使用的模型，自动标记功能可能会**有时失败**或误解对话。当这种情况发生时，最好**手动标记你的聊天**以确保反馈准确。

- **如何手动标记**：在评价回答时，你可以根据对话的上下文添加自己的标签。

不要跳过这一步！标记非常强大，因为它允许你**按特定主题重新排名模型**。例如，你可能想知道哪个模型在回答技术支持问题时表现最佳，而不是一般的客户咨询。

以下是重新排名的一个示例：

![按主题重新排名的排行榜](/img/evaluation/leaderboard-reranked.png)

---

### 旁注：用于模型微调的聊天快照

每当你评价一个模型的回答时，Open WebUI会*捕获该聊天的快照*。这些快照最终可以用于**微调你自己的模型**——因此你的评估将为AI的持续改进做出贡献。

*（敬请期待此功能的更多更新，它正在积极开发中！）*

---

## 总结

**简而言之**，Open WebUI的评估系统有两个明确的目标：
1. 帮助你**轻松比较模型**。
2. 最终找到最适合你个人需求的模型。

从根本上说，该系统旨在让AI模型评估变得**简单、透明且可定制**，适用于每个用户。无论是通过竞技场模式还是常规聊天互动，**你完全掌控着决定哪个AI模型最适合你特定用例的权利**！

**一如既往**，所有数据都安全保存在**你的实例**中，除非你特意**选择加入社区共享**，否则不会共享任何数据。你的隐私和数据自主权始终被优先考虑。